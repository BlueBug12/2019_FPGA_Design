{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "keras.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BlueBug12/2019_FPGA_Design/blob/master/hw13_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfMKEx6zKSHG"
      },
      "source": [
        "# Keras\n",
        "\n",
        "[Keras](https://keras.io/) 是一種較 Tensorflow 更高階的深度學習框架，可以使用更少量的程式碼來建立深度學習模型，可以先熟悉 [Tensorflow 單元](/notebooks/unit/tensorflow/tenforflow.ipynb)後再來閱讀本單元。Keras 使用更低階的深度學習框架作為後端引擎，目前支援如 CNTK、Tensorflow、Theano 等知名框架。本單元將介紹 Keras 中 Model 與 Layer 的用法，並實作一個圖片分類器。\n",
        "\n",
        "## 1. Model & Layer\n",
        "\n",
        "在 Keras，可以宣告一個 [Model](https://keras.io/models/about-keras-models/) 物件，並透過加入一層一層的 [Layer](https://keras.io/layers/about-keras-layers/) 來建構一個神經網路，神經網路的運算(例如訓練)都可以透過該 Model 物件來操作。下方程式區段使用了 Keras 中常見的 [Sequential Model](https://keras.io/models/sequential/)，並加入了四種 Layer：\n",
        "\n",
        "1. [Convolutional Layer](https://keras.io/layers/convolutional/)：卷積層在影像、圖片應用上，表現比全連結層(Keras 的 Dense Layer 更為優異)，參考[卷積神經網絡介紹](https://medium.com/@yehjames/4f8249d65d4f)。\n",
        "2. [Pooling Layer](https://keras.io/layers/pooling/#maxpooling2d)：池化層的工作是降採樣(down sampling)，以下方程式區段使用的 MaxPooling 為例，將每個 2x2 降採樣為該區域的最大值。\n",
        "3. [Flatten Layer](https://keras.io/layers/core/#flatten)：將原本多維度的資料拉平成一維，目的是讓前一層的輸出可以接到下一層(通常是全連接層)的輸入。\n",
        "4. [Dense Layer](https://keras.io/layers/core/#dense)：全連結層。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeSIMwJFKSHH",
        "outputId": "5bb70b4d-e81a-4853-abf8-4b0fefa79195"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "\n",
        "model = Sequential() # Declare a sequential model\n",
        "\n",
        "# Add a 2D convolutional layer with 64 nodes, a 3x3 filter and relu as avtivation function\n",
        "# After this layer, `model.output_shape` is (None, 62, 62, 64)\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
        "\n",
        "# Add a 2D max pooling layer that pools the maximun value every 2x2 area\n",
        "# After this layer, `model.output_shape` is (None, 31, 31, 64)\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add a flatten layer\n",
        "# After this layer, `model.output_shape` is (None, 61504)\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a dense layer with 32 nodes and sigmoid as activation function\n",
        "# After this layer, `model.output_shape` is (None, 32)\n",
        "model.add(Dense(32, activation='sigmoid'))\n",
        "\n",
        "# See `model`\n",
        "model.summary() "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 62, 62, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 31, 31, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 61504)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                1968160   \n",
            "=================================================================\n",
            "Total params: 1,969,952\n",
            "Trainable params: 1,969,952\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPr4u6ADKSHI"
      },
      "source": [
        "## 2. CIFAR-10\n",
        "\n",
        "CIFAR 的全名為 Canadian Institute for Advanced Research，是由加拿大政府出資並由多位科學家、工程師收集而成的圖片資料庫。[CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html) 包含 60000 張 32x32x3 的 RGB 彩色圖片，其中 50000 張為訓練資料，10000 張為測試資料。CIFAR-10 有 10 種類別，0~9 分別對應為：\n",
        "\n",
        "```\n",
        "airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
        "```\n",
        "\n",
        "![CIFAR-10](./cifar_10.png)\n",
        "\n",
        "Keras 提供[整理好的 CIFAR-10 資料](https://keras.io/datasets/#cifar10-small-image-classification)，只要透過 `import` 就可以拿到對應的訓練與測試資料。用法如下："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "Yjtheix7KSHJ",
        "outputId": "2792fa46-d47f-4c6f-ff23-8c3164f0d26a"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# see the data shape\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)\n",
        "print()\n",
        "\n",
        "# show the i-th sample of the cifar-10 training set, try a different `i`\n",
        "i = 0\n",
        "plt.imshow(x_train[i])\n",
        "print('The label of training sample %d is %s.' % (i, y_train[i]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "(50000, 32, 32, 3) (50000, 1)\n",
            "(10000, 32, 32, 3) (10000, 1)\n",
            "\n",
            "The label of training sample 0 is [6].\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfMklEQVR4nO2da2yc53Xn/2dunOGdFC+SKNmy5UvtNLbiqIbXyXaTBi3coKgTYJFNPgT+EFRF0QAN0P1gZIFNFtgPyWKTIB8WWSgbt+4im8vm0hiFsW1qpDDaFK7l2PG9tizLkSiKokRS5HCGcz37YcZb2fv8H9IiOVTy/H+AoOF7+LzvmWfe877zPn+ec8zdIYT41Sez2w4IIXqDgl2IRFCwC5EICnYhEkHBLkQiKNiFSITcVgab2X0AvgogC+B/uPsXYr+fz+e9r1gM2lqtFh2XQVgezBo/ViHHr2P5iC2XzVKbWfiAZpFrZsTHZpO/55ggmo35SKTUtrf5sdr8aJaJvIEI7Xb4vcV8j+4v4r9FJpnZMhE/shn+ebJzAADaERnbYycCGxPdX5jF5VWUK+vBg111sJtZFsB/A/DbAM4CeNLMHnH3F9mYvmIRR+56b9C2vLxIj9WXCX/Q4wU+Gdft6ae2yfEBapsYHaS2QjYf3J7rK9ExyPIpXlxaprZ6k7+3sdERasu0GsHttVqNjllfX6e2Yil8cQaAFvjFqlItB7ePjA7TMXC+v3qtTm1ZhD8XgF9chgb55zwwwM+PfJ7PRzXio8duCJnwORJ7z00PXzy++I3v88NwDzbkbgAn3f2Uu9cBfBvA/VvYnxBiB9lKsM8AOHPFz2e724QQ1yBbembfDGZ2DMAxAOjr69vpwwkhCFu5s88COHjFzwe6296Cux9396PufjSX589WQoidZSvB/iSAm83sBjMrAPg4gEe2xy0hxHZz1V/j3b1pZp8G8NfoSG8PufsLsTHr6+t44cXwryxfvEjHjZMFUNvDV0YnWkPUZqUpaltrc1Wg3AqvkLsV6JjKOl9RrVT5CnmjxaWmixHNsZgL+9hs8v1lyWowEH/0qqyvUVuzHX7ftr6HjslEVLlGRE0o5fh5UCYr2outJh3T389X4y3Dv50aUWsAABE5r7IeVlCajfB2AMjmwp9LY71Kx2zpmd3dHwXw6Fb2IYToDfoLOiESQcEuRCIo2IVIBAW7EImgYBciEXb8L+iuJAOglCOyUeSP664nEtuhaZ4QMjU5Tm2lmLQSyWqq1sIJI+sNLgt5ZH+FUiSBJpII421+vJHxcAJQs8H3V8hzPyLJiMgW+IdWq4fnqtHk89Ef2V9ugPtYjIxrWlgezESy6JqRDLVYpuXgAE++Kq9VqK3RDEtssYTD1ZXLwe3taPaoECIJFOxCJIKCXYhEULALkQgKdiESoaer8WaOooUTEIaGuCu3zIwFt+8p8cyJfJuXWiov8uSUVptf/6qVsO8ZngeD4UiZq1xkFXn58iofF/nUxofCK8KrKzxppR5JaKmSJA0gXldtkJR2atR5okamxd9YPpKQ0yKluAAgR5bPazU+ppDnH2imzRNoauUlagNJogKAPnIaN9tcMbi8FlZkWpF6grqzC5EICnYhEkHBLkQiKNiFSAQFuxCJoGAXIhF6Kr3lzDDWFz5kKSKtjJAkiMlhXvOrRdoPAYj0MQGyuUghNFJHrNaOSD8RnSwXScZo1bhE5Vl+jb5wIdxlptXg73q1wpM0Ki0uUw6WIt1daqT9E/h7zhiXjbJ9kU4sa1xm7c+HfcxFWiutR+oGVhtcemtHmnYtl7mPy5Xw+VMmUi8ArDfC50A9UmtQd3YhEkHBLkQiKNiFSAQFuxCJoGAXIhEU7EIkwpakNzM7DWAVHTWr6e5HowfLGiZHwxLKUJ5LXsVi2JbJcqmjFKnv1mhyGaodyeTqtKH//6lH6sW16lyWa3skoywieXmOZ2Wt1sMZbK0Wn99KpNVUM2JbXeP+zy6G/chn+P6Gy3zuG+d5e7DqZS4dXjdxU3D71NQBOsaGwvXdAKC2dInaymWePXh5lUtvFy+HZdbTZ7gfrWw4dGt1Ltdth87+QXfnn4QQ4ppAX+OFSIStBrsD+Bsze8rMjm2HQ0KInWGrX+Pf7+6zZjYF4Mdm9rK7P37lL3QvAscAoBh5LhdC7CxburO7+2z3/wsAfgjg7sDvHHf3o+5+tJDTU4MQu8VVR5+ZDZjZ0JuvAfwOgOe3yzEhxPayla/x0wB+2G2XlAPwv9z9/8QG5HNZ7J8MFyIcLnDJYLA/LDVZRLpCJAPJItlmtSqXcTJEltszxNtQDQzwbK2Vy1zEGBnmGWWrkSKQb8yG91mu8UeoAp8OzPRHsvbyPDPv9KVw9l3NI0VCI1lvI8ND1Hbv7VzxXZkLy6xeiRxrgmdT1ip8Psplfu/sy/N9Htwbfm9TU9N0zPxKWMq79Mp5Ouaqg93dTwG482rHCyF6ix6ihUgEBbsQiaBgFyIRFOxCJIKCXYhE6G3ByaxhfCicjZarh6UaAOjLh93s7wv3NQOAWpXLU41Iv67R0XBfOQBwUqSw3uLXzEYjUgxxkPeBO7cQ7uUFAK+9wbOhFlbD7y1SuxDXR3rmfeRfH6G2A/u4/9976lRw+z+e5NJQs80z/XIZLpWtLi9QW6UcnsehIS6FocWz74pFPq5AsjMBoN/4uGYr/OFcd3A/HTO0GO4F+OzrfC50ZxciERTsQiSCgl2IRFCwC5EICnYhEqG3q/G5HKbG9wRt1UW+ap2xsJtl0jYHAKqxWlwWqccWaZPErozVBl9FHh3jCS31Fl9hPnX2HLUtrnAfWX26bKRl1HCR728qF171BYDiIlcMbh7eG9w+N879mF++QG21Cp/jp195hdoypB1SYyDSumqEJ6Agw0NmZISrQ0PtSLspUqfQ6yt0zCGSUNaX5/OrO7sQiaBgFyIRFOxCJIKCXYhEULALkQgKdiESocfSWx5jE5NB29ggb9eUyYSTCJZXluiYxlqZ768Va//EC7I5ScgZHOR15hrgtpdOcclorcZbCRWLfdxWCPtYGuCy0FiWy5RPnZyntmadnz61kbD0NjnG58PA5bBGk0uzlTqvhbdGas3Vm/w9W0RKjXQHQz4TaR2WidTey4XnsVnj0qYT2ZbkagHQnV2IZFCwC5EICnYhEkHBLkQiKNiFSAQFuxCJsKH0ZmYPAfg9ABfc/de728YBfAfAIQCnAXzM3bkO9i97A4iMZpH2OIy+SD2wfoSzggAgF7nGZTKRenJElusr8fZPF8/zrLHKRT5lN45ziarGVSgUicR26+EZOiYT2WEzy+d4JSJ95rLhOnlDBf657Bk7TG2Hb76O2l7/xZPU9vIrs8HthVxE1nIu2zabPGQyJOMQAPIFPo/tdvi8akd0PrPweRpRBjd1Z/9zAPe9bduDAB5z95sBPNb9WQhxDbNhsHf7rS++bfP9AB7uvn4YwEe22S8hxDZztc/s0+4+1319Hp2OrkKIa5gtL9B5p5g6/SM9MztmZifM7MRqJfKwKYTYUa422OfNbB8AdP+n9YTc/bi7H3X3o0P9fNFJCLGzXG2wPwLgge7rBwD8aHvcEULsFJuR3r4F4AMAJszsLIDPAfgCgO+a2acAvAHgY5s5WNsd1fVwcT1r8MwlIJyhtLbGC/LVG/w61szwbxjlCpfKVoht5iCfRm/y/V0/wYWSw/u5VFNZ5+NmbrkzuL3g/BFq6TIv3FkaDRcIBQBc4plcB/fuC25fXuPZfDf+2s3UNjzGs/aGx26jtqWF8PwvXeYttPIReTDjPOOw0Y5kU/JkSrQa4fM7kkRHW5FFkt42DnZ3/wQxfWijsUKIawf9BZ0QiaBgFyIRFOxCJIKCXYhEULALkQg9LTjpcLQsLE94ixcAZDJDqciLVA4Ocanm3AKX+V4/u0BtuXzYj8I878u2Ps/3d/MUl9c+9AEuQ702+/ZUhX9haCZc0HNiT7gAJABcWOBFJUdHIzJUm/tfIAUWLyyEs9AAIFdcpraF5Tlqm53jWWr5fPg8GB3mWli1ygUsz/H7o0W0snZElstYeJxFMjAjbQL5cd75ECHELyMKdiESQcEuRCIo2IVIBAW7EImgYBciEXoqvWWzGYyODgZtzRyX3srlcMaWN7iccXmVZzW98QsuNZXLXMYpFcPXxrnXefbddJEXIZyZuZ7aRvffQG351UgKFSnCeeDOu/mQ81wOKzW5dNgCz6RbWwvb9vWHpUEAqLf4+7KB8HkDAAcG9lPb0GhYcly9dJ6OuTB/idoaxuXG9TovYokM18oG+sJZmPVqRFIkBSyNyHiA7uxCJIOCXYhEULALkQgKdiESQcEuRCL0dDW+3WpidTm80pmr81ptedLqBrwEGnJZbqyU+Ur92BBP/BgdCK+aVpf4avzUfl7DbeaOf0Ntz5+tU9srJ7nt3n3jwe3Ly3zM9OFw3ToAyKBCbfUaX6kf9fDK+soFvtJdqvNaePvGw+8LAJZbvC5c/o6x4PZqJLHmHx59hNrOnuHvORtp8RRrzMTybhqxNmWN8FyxpDFAd3YhkkHBLkQiKNiFSAQFuxCJoGAXIhEU7EIkwmbaPz0E4PcAXHD3X+9u+zyAPwDwpg7xWXd/dDMHzBIFohX5o38nskWGtIUCgJZx6W2JKzxYWYnUH6uF5at9I1yu+40PfpDaDtx6D7X94M8eora9kaSQbD1cX2/21Gt8fzfeTm3FPTdR24BzubSyGO71WWqHpTAAqFe5zHdxldtGJ3nS0J69h4Lbq+VhOibDTWgVePJPrAZdo8GlT2uGE7rMeaJXsxkO3a1Kb38O4L7A9q+4+5Huv00FuhBi99gw2N39cQC8nKkQ4peCrTyzf9rMnjWzh8yMfzcTQlwTXG2wfw3AYQBHAMwB+BL7RTM7ZmYnzOxEucKfW4QQO8tVBbu7z7t7y93bAL4OgJZBcffj7n7U3Y8O9vOqLUKIneWqgt3M9l3x40cBPL897gghdorNSG/fAvABABNmdhbA5wB8wMyOAHAApwH84WYOZgCMKAMtksUD8DY4kU488Gpkf5ESbuN7eNuovf1hqe+uo7fQMbfdy+W1pQtcbuxr8sy8Gw8coLY2eXN7p3jtt+Y6lzArkWy5epOPa1TDp1YLXDZ8bfYstT33/Alqu/ce7uOeveGsw5XVsDQIAKRjFABg4hCXWduxdk31iIxGJN3LC7wdVm017GSbZBsCmwh2d/9EYPM3NhonhLi20F/QCZEICnYhEkHBLkQiKNiFSAQFuxCJ0NOCk+5Am2T4VGtcMiiQLK9cjhf4y2a4HHPTXv7XvcUSv/4duv5gcPud7+eZbftuvYPanvnHP6O26w5yH/e+693UVpg8HNye6x+hYyrrXAKsrvDMtvlzZ6htaT4so7UaPHutNBQu6AkAExP8sz5z7mlqm943E9zerESyLKu8jZOtLVFby8MZhwDgTHMGUOoLv7fCXv6eV/pIJmgkonVnFyIRFOxCJIKCXYhEULALkQgKdiESQcEuRCL0VHozM+Sz4UMuRQoKttbDMkOpv0THZDNc6piKZLadmeOZRofvCpXiAw68O7y9A5fQGqtr1DYyxKWyyVuOUNtaLtwT7YWnn6RjalXux8oKn4+Ls7+gtmwrLH0Wi/yUm7khLJMBwB238MKXzSzPRMtnR8PbCzwrMrfOi0pW3pilNiYrA0Azclstk76E/Xv4+5omPQTz+Uh/OO6CEOJXCQW7EImgYBciERTsQiSCgl2IROhtIky7jVo1vNLZ38ddsWJ4tTKf4TXQvMVtpUHeGur3/93vU9u9v/uh4PbhiWk6Zv7US9SWjfi/vMpr0C2c/mdqO7caXhH+u7/8SzpmsMQTLtZrPGFk7zRXDIaHwivJr5/lyTP1yHyM7z9Ebbe8+73UhlZfcPPiMq93VyHqDwAsVbmP5vwcXq/yRK8yadnkZa4K3BYWGdDmIpTu7EKkgoJdiERQsAuRCAp2IRJBwS5EIijYhUiEzbR/OgjgLwBMo9Pu6bi7f9XMxgF8B8AhdFpAfczdeYEuAA5H20ltuDZPIrBmWLZoeqTFU6TmV7FvmNqOvJfLOH35sET14jO8BtrSudeorVbj0srq0iK1nTn5IrWVPZwclG/xYw3muBQ5XOTJGJNjXHqbmz8f3N6MtPmqrHKZ78zrPOkGeIFayuVwDb1ijp8fzb4parvU5OdOqcRr6PUP8aStUi4sD65WVuiYZjssAUaUt03d2ZsA/tTdbwdwD4A/NrPbATwI4DF3vxnAY92fhRDXKBsGu7vPufvPuq9XAbwEYAbA/QAe7v7awwA+slNOCiG2zjt6ZjezQwDeA+AJANPuPtc1nUfna74Q4hpl08FuZoMAvg/gM+7+locJd3eQxwUzO2ZmJ8zsxFqV13IXQuwsmwp2M8ujE+jfdPcfdDfPm9m+rn0fgGDDa3c/7u5H3f3oQKmwHT4LIa6CDYPdzAydfuwvufuXrzA9AuCB7usHAPxo+90TQmwXm8l6ex+ATwJ4zsye6W77LIAvAPiumX0KwBsAPrbxrhxAWEZrN/lX/Fw+XDOuFan5VQfPTpoe4XXh/vqRv6K28emwxDO1L9wWCgDqFZ69ls+HJRcAGBzgEk8uw6WyASIP7p0K1ywDgOoqV0xLWe7jpYWL1Naohz+boSKXoOplLr29+vQJapt7+RVqqzVJS6Y8n8NWbH4PcCkSA/wczvRx6bNIZLQx8Lm67V03BLeXiqfomA2D3d3/HgDL+QvnfAohrjn0F3RCJIKCXYhEULALkQgKdiESQcEuRCL0tOAk3NBuhxf2C5HMq2KOFOvL8MKAHmkJ1K7zzKuLF8PZWgBQXgjbSg2endQGf1/jY1wOG90/SW3NVo3aZs+FffRIPlQmw0+DepNLmFnjhSoHimG5lCQwdvYXM0ayGFt1Lm9myPm2UuFyY72PyHUAhvbzuV8r8VZZq20uy62vhe+5e4ZvpGMmiJSay/PPUnd2IRJBwS5EIijYhUgEBbsQiaBgFyIRFOxCJEJvpTcYMhbOoir28QwfJxlsA6WwvAMAA0MT1FZp8AykPUM85z5H/Khfnqdj2hm+v0qeS03T0+GsJgBo17mMc+sdB4Lbf/qTx+iYuleoLW9c3qyW+bjhoXDWXiHHT7msRfqhrfPP7PU5LqMtL4c/s5qt0TGTt/B74MxoJGvP+We9dJHPVWE9LGEOzEQyFSvhrMJ2RL3UnV2IRFCwC5EICnYhEkHBLkQiKNiFSISersZnDCjkwteXSo0nGGRJC6J2pD5apcGTGbJ5nlTRV+Crrfl82I9CP2+DNDLME3LOL/BV/MpMeFUdAKYO3kRtsxfCdeHe9Rvvo2PKC+eo7dQrvLXSWpknfuSy4fkfGeG19YzUJwSAuVnu4y/eiCTC9IXnf3iaKzmT4xEfI6qALfLPemyJh9rM1Hhw+4FRfg6cfDGc8FSr8iQv3dmFSAQFuxCJoGAXIhEU7EIkgoJdiERQsAuRCBtKb2Z2EMBfoNOS2QEcd/evmtnnAfwBgIXur37W3R+NHixnmJ4MX18aly7RcdVWWJJZ47kM8AxvDZWLJGMMD/PkgwJprVRd4zXoSpGaYKhz24mf/pTabryVS3Znz4YlmUykXl9/H68ll43Im6USl5rWymHprVrlkmgz0gJssMT9uPc9t1BbkSTkNLO8tl6rwZNWqme49JZZLVLbVP8Qtb3nlneFx4zyLuhPzb0e3N5s8Pe1GZ29CeBP3f1nZjYE4Ckz+3HX9hV3/6+b2IcQYpfZTK+3OQBz3derZvYSgJmddkwIsb28o2d2MzsE4D0Anuhu+rSZPWtmD5kZb40qhNh1Nh3sZjYI4PsAPuPuKwC+BuAwgCPo3Pm/RMYdM7MTZnZipcKfyYQQO8umgt3M8ugE+jfd/QcA4O7z7t5y9zaArwO4OzTW3Y+7+1F3Pzrczyt5CCF2lg2D3cwMwDcAvOTuX75i+74rfu2jAJ7ffveEENvFZlbj3wfgkwCeM7Nnuts+C+ATZnYEHTnuNIA/3GhHhYLhuoPhu/uIcdni5JmwFDK/wLPX6i0u1QwO8re9VuEZVK12Obg9G7lmLi5wSXG1zGWS9Qb3I+vcNjQYXjqZP79Ix5xd43JS27lkNz3JZUprh7OvlpZ5vbi+Af6ZjY5w6aqQ5fNfqxMJNsflxrUa31+9HGl51ebjbjq4l9r27w3P45mzXGK9tBCOiWakhdZmVuP/HkDoE49q6kKIawv9BZ0QiaBgFyIRFOxCJIKCXYhEULALkQg9LTiZzRmGx0jmGJESAGBsKhs2DPCigRfneQHL9Uj7pFyBFxtkw9oNnmHXaHE/Lle5DDUQyfJar3CprLoeLjhZj/jYitjcydwDKK9E2j8Nhwt3Dg/z4pzVKt/fxUt8rgYHefadZcL3M2ty2baQ40VH+7hCjEKBz9Whmw5RW7US9uXxx1+kY5595UJ4X+tcztWdXYhEULALkQgKdiESQcEuRCIo2IVIBAW7EInQU+nNzJArhg9ZHOa57uOD4WtSrsplrXyJZ/+sRPpuocWvf6XiVHhInh+rVeP90Ar93I98js9HNsslx5qHfak3uNzokcw24woVvM4lwBYx5SPZZihwuXF5iUtv1TrvbzYyGpZSc0SSA4BMZO4r4NLW/MVValuKZDiuroWzGP/2717mxyIq5Xpd0psQyaNgFyIRFOxCJIKCXYhEULALkQgKdiESoafSW7ttKLOCfdlBOm5wIKzj5EtcFxqIpCeNjHCprLzCe5GVV8IFAMuVSNbbOrcNFXjBxiLpKwcAzRqXHHO58PW7ELms5/t4tpYZH9gfKdyZIaZmi0tDhVKkB98olxsXF7nktUqkyOFxPveVSM+5V0/zAqIvP3eG2qbHeTbl9AHy3jL8PJ0gBTjnV7kMqTu7EImgYBciERTsQiSCgl2IRFCwC5EIG67Gm1kRwOMA+rq//z13/5yZ3QDg2wD2AHgKwCfdPdqmtV4Hzr4RttWW+er50GR4BbdYiiRA8MV9jI/zt11e43XQlpfDtqVLPHFiiS/eItvmq+Bt50pDq8VX+NEO22JXdcvwRJhsjs9VNZI05GTRPU/aQgFAs8JbVLUi9elakeSa5XJ4HOsKBQCLEUXm9En+gS5fWqO2+ho/4N6RcGuo266foWOYi6+eX6FjNnNnrwH4LXe/E532zPeZ2T0AvgjgK+5+E4AlAJ/axL6EELvEhsHuHd7saJjv/nMAvwXge93tDwP4yI54KITYFjbbnz3b7eB6AcCPAbwGYNn9/31ZOwuAf+cQQuw6mwp2d2+5+xEABwDcDeDXNnsAMztmZifM7MTlMi92IITYWd7Rary7LwP4CYB/BWDUzN5cvTkAYJaMOe7uR9396MhgpMK+EGJH2TDYzWzSzEa7r0sAfhvAS+gE/b/t/toDAH60U04KIbbOZhJh9gF42Myy6Fwcvuvuf2VmLwL4tpn9ZwBPA/jGRjtyy6GVnwjaGoWjdFytHU78yDTDrY4AoDjC5aTRSf4NYyzDEzXGK+HEhOVF3i5o+SKX16prfPpbTS7nwfk1ut0M+7he5Y9QhUKk3l2O+7+6zhM1quSRLR9RZ4cy4eQOAGhnuKTUaPB57BsIS5jFPK93N1rgPt6IUWp79528DdWtd9xJbYduuim4/e57uNx49lw5uP0fXuMxsWGwu/uzAN4T2H4Kned3IcQvAfoLOiESQcEuRCIo2IVIBAW7EImgYBciEcwj2VXbfjCzBQBv5r1NAOA6Qe+QH29FfryVXzY/rnf3yZChp8H+lgObnXB3Lq7LD/khP7bVD32NFyIRFOxCJMJuBvvxXTz2lciPtyI/3sqvjB+79swuhOgt+hovRCLsSrCb2X1m9s9mdtLMHtwNH7p+nDaz58zsGTM70cPjPmRmF8zs+Su2jZvZj83s1e7/Y7vkx+fNbLY7J8+Y2Yd74MdBM/uJmb1oZi+Y2Z90t/d0TiJ+9HROzKxoZv9kZj/v+vGfuttvMLMnunHzHTOLpEYGcPee/gOQRaes1Y0ACgB+DuD2XvvR9eU0gIldOO5vArgLwPNXbPsvAB7svn4QwBd3yY/PA/j3PZ6PfQDu6r4eAvAKgNt7PScRP3o6JwAMwGD3dR7AEwDuAfBdAB/vbv/vAP7onex3N+7sdwM46e6nvFN6+tsA7t8FP3YNd38cwNvrJt+PTuFOoEcFPIkfPcfd59z9Z93Xq+gUR5lBj+ck4kdP8Q7bXuR1N4J9BsCV7S53s1ilA/gbM3vKzI7tkg9vMu3uc93X5wFM76IvnzazZ7tf83f8ceJKzOwQOvUTnsAuzsnb/AB6PCc7UeQ19QW697v7XQB+F8Afm9lv7rZDQOfKjs6FaDf4GoDD6PQImAPwpV4d2MwGAXwfwGfc/S2laXo5JwE/ej4nvoUir4zdCPZZAAev+JkWq9xp3H22+/8FAD/E7lbemTezfQDQ/f/Cbjjh7vPdE60N4Ovo0ZyYWR6dAPumu/+gu7nncxLyY7fmpHvsd1zklbEbwf4kgJu7K4sFAB8H8EivnTCzATMbevM1gN8B8Hx81I7yCDqFO4FdLOD5ZnB1+Sh6MCdmZujUMHzJ3b98hamnc8L86PWc7FiR116tML5ttfHD6Kx0vgbgP+ySDzeiowT8HMALvfQDwLfQ+TrYQOfZ61Po9Mx7DMCrAP4WwPgu+fE/ATwH4Fl0gm1fD/x4Pzpf0Z8F8Ez334d7PScRP3o6JwDuQKeI67PoXFj+4xXn7D8BOAngfwPoeyf71V/QCZEIqS/QCZEMCnYhEkHBLkQiKNiFSAQFuxCJoGAXIhEU7EIkgoJdiET4vyrWWZ/xQ9u6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZsasu22KSHJ"
      },
      "source": [
        "## 3. 範例模型\n",
        "\n",
        "以下使用 Keras 實作兩種深度學習模型來進行 CIFAR-10 圖片分類。其中 DNN 只使用全連接層，而 CNN 多使用了卷積層。相較於 [Tensorflow 單元](/notebooks/unit/tensorflow/tenforflow.ipynb)的 MNIST 資料，CIFAR-10 的圖片比較複雜且為彩色，更能發揮卷積層的效果。也因此相較於 DNN，CNN 應該更容易得到好的結果。注意比較以下兩個程式區段，使用 CNN 時不需要將圖片 reshape 為一維向量。當然，CNN 也能處理 reshape 過的一維向量。CNN 的初學者可以參考以下連結：\n",
        "\n",
        "* [A Beginner's Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)\n",
        "* [深度學習(2)--使用Tensorflow實作卷積神經網路(Convolutional neural network，CNN)](http://arbu00.blogspot.tw/2017/03/2-tensorflowconvolutional-neural.html)\n",
        "\n",
        "![Convolutional Neural Network](https://adeshpande3.github.io/assets/Cover.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8aHoc-8KSHJ",
        "outputId": "893163e2-b43e-4257-d06f-6b1b82d7b7e4"
      },
      "source": [
        "# DNN\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "\n",
        "x_tr_dnn = x_train[:10000].astype('float32')\n",
        "x_te_dnn = x_test.astype('float32')\n",
        "\n",
        "# note that the CNN version does not need to reshape the input\n",
        "x_tr_dnn = x_tr_dnn.reshape(-1, 3072)\n",
        "x_te_dnn = x_te_dnn.reshape(-1, 3072)\n",
        "\n",
        "# normalize\n",
        "x_tr_dnn /= 255\n",
        "x_te_dnn /= 255\n",
        "\n",
        "# one-hot encoding\n",
        "y_tr_dnn = to_categorical(y_train[:10000], num_classes=10)\n",
        "y_te_dnn = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# options\n",
        "epochs = 20\n",
        "batch_size = 128 \n",
        "learning_rate = 0.001\n",
        "\n",
        "# model\n",
        "model = Sequential()\n",
        "model.add(Dense(100, activation='relu', input_shape=(3072,)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "# train\n",
        "model.fit(x_tr_dnn, y_tr_dnn, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(x_te_dnn, y_te_dnn))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 2.2120 - accuracy: 0.2042 - val_loss: 2.0926 - val_accuracy: 0.2316\n",
            "Epoch 2/20\n",
            "79/79 [==============================] - 1s 7ms/step - loss: 1.9573 - accuracy: 0.2949 - val_loss: 1.9091 - val_accuracy: 0.3138\n",
            "Epoch 3/20\n",
            "79/79 [==============================] - 1s 6ms/step - loss: 1.8674 - accuracy: 0.3358 - val_loss: 1.9118 - val_accuracy: 0.3128\n",
            "Epoch 4/20\n",
            "79/79 [==============================] - 1s 6ms/step - loss: 1.8335 - accuracy: 0.3501 - val_loss: 1.8260 - val_accuracy: 0.3579\n",
            "Epoch 5/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.8034 - accuracy: 0.3594 - val_loss: 1.8228 - val_accuracy: 0.3544\n",
            "Epoch 6/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.7752 - accuracy: 0.3773 - val_loss: 1.8482 - val_accuracy: 0.3444\n",
            "Epoch 7/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.7495 - accuracy: 0.3829 - val_loss: 1.7948 - val_accuracy: 0.3688\n",
            "Epoch 8/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.7367 - accuracy: 0.3849 - val_loss: 1.7592 - val_accuracy: 0.3772\n",
            "Epoch 9/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.7142 - accuracy: 0.3925 - val_loss: 1.8237 - val_accuracy: 0.3453\n",
            "Epoch 10/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.7090 - accuracy: 0.3924 - val_loss: 1.7827 - val_accuracy: 0.3654\n",
            "Epoch 11/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.6974 - accuracy: 0.3998 - val_loss: 1.7657 - val_accuracy: 0.3695\n",
            "Epoch 12/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.6661 - accuracy: 0.4112 - val_loss: 1.7432 - val_accuracy: 0.3828\n",
            "Epoch 13/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.6536 - accuracy: 0.4115 - val_loss: 1.7574 - val_accuracy: 0.3777\n",
            "Epoch 14/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.6493 - accuracy: 0.4172 - val_loss: 1.8048 - val_accuracy: 0.3545\n",
            "Epoch 15/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.6519 - accuracy: 0.4096 - val_loss: 1.7569 - val_accuracy: 0.3755\n",
            "Epoch 16/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.6296 - accuracy: 0.4205 - val_loss: 1.7468 - val_accuracy: 0.3858\n",
            "Epoch 17/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.6234 - accuracy: 0.4214 - val_loss: 1.7553 - val_accuracy: 0.3871\n",
            "Epoch 18/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.6220 - accuracy: 0.4280 - val_loss: 1.7235 - val_accuracy: 0.3844\n",
            "Epoch 19/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.5928 - accuracy: 0.4381 - val_loss: 1.7207 - val_accuracy: 0.3913\n",
            "Epoch 20/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.5994 - accuracy: 0.4382 - val_loss: 1.7116 - val_accuracy: 0.3953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa45c125c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4hx2ylaKSHJ",
        "outputId": "854d1edd-93ba-4ca7-e8d5-78ea5717da02"
      },
      "source": [
        "# CNN\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "x_tr_cnn = x_train[:50000].astype('float32')\n",
        "x_te_cnn = x_test.astype('float32')\n",
        "\n",
        "# normalize\n",
        "x_tr_cnn /= 255\n",
        "x_te_cnn /= 255\n",
        "\n",
        "# one-hot encoding\n",
        "y_tr_cnn = to_categorical(y_train[:50000], num_classes=10)\n",
        "y_te_cnn = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# options\n",
        "epochs = 100\n",
        "batch_size = 128 \n",
        "learning_rate = 0.001\n",
        "\n",
        "# model\n",
        "model = Sequential()\n",
        "\n",
        "# the input shape for cifar-10 is (32, 32, 3)\n",
        "# use `Conv2D(#neurons, (filter_size))` to add convolutionary layers\n",
        "model.add(Conv2D(64, (3, 3), input_shape=(32, 32, 3),padding='same', activation='relu'))\n",
        "\n",
        "# use `MaxPooling2D()` to add pooling layers\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2),strides=(2,2)))\n",
        "model.add(Conv2D(128, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2),strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(256, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2),strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(256, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2),strides=(2,2)))\n",
        "'''\n",
        "model.add(Conv2D(512, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2),strides=(2,2)))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2),strides=(2,2)))\n",
        "'''\n",
        "#model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# TODO: add more convolutionary and/or pooling layers here\n",
        "\n",
        "# in practice, fully-connected layers are added after convolutionary and pooling ones \n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "# train\n",
        "callback = EarlyStopping(monitor=\"val_accuracy\", patience=10, verbose=1, mode=\"auto\")\n",
        "model.fit(x_tr_cnn, y_tr_cnn, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(x_te_cnn, y_te_cnn), callbacks=[callback])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 2.3022 - accuracy: 0.1071 - val_loss: 2.3016 - val_accuracy: 0.1387\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 2.2997 - accuracy: 0.1256 - val_loss: 2.2981 - val_accuracy: 0.1336\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 2.2655 - accuracy: 0.1547 - val_loss: 2.2053 - val_accuracy: 0.2056\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 2.1256 - accuracy: 0.2087 - val_loss: 2.1603 - val_accuracy: 0.1996\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 2.0414 - accuracy: 0.2391 - val_loss: 1.9843 - val_accuracy: 0.2717\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.9868 - accuracy: 0.2648 - val_loss: 1.9251 - val_accuracy: 0.2931\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.9380 - accuracy: 0.2815 - val_loss: 1.9191 - val_accuracy: 0.2932\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.8770 - accuracy: 0.2996 - val_loss: 1.7967 - val_accuracy: 0.3336\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.8229 - accuracy: 0.3214 - val_loss: 1.7444 - val_accuracy: 0.3496\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.7694 - accuracy: 0.3391 - val_loss: 1.6913 - val_accuracy: 0.3657\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.7179 - accuracy: 0.3572 - val_loss: 1.7095 - val_accuracy: 0.3720\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.6640 - accuracy: 0.3785 - val_loss: 1.7840 - val_accuracy: 0.3480\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.6240 - accuracy: 0.3951 - val_loss: 1.5462 - val_accuracy: 0.4225\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.5951 - accuracy: 0.4073 - val_loss: 1.5929 - val_accuracy: 0.4012\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.5604 - accuracy: 0.4209 - val_loss: 1.5847 - val_accuracy: 0.4230\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 8s 22ms/step - loss: 1.5272 - accuracy: 0.4329 - val_loss: 1.5066 - val_accuracy: 0.4348\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.4879 - accuracy: 0.4474 - val_loss: 1.4501 - val_accuracy: 0.4622\n",
            "Epoch 18/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.4511 - accuracy: 0.4642 - val_loss: 1.3810 - val_accuracy: 0.4841\n",
            "Epoch 19/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.4175 - accuracy: 0.4775 - val_loss: 1.3503 - val_accuracy: 0.4958\n",
            "Epoch 20/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.3809 - accuracy: 0.4904 - val_loss: 1.3659 - val_accuracy: 0.5039\n",
            "Epoch 21/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.3465 - accuracy: 0.5053 - val_loss: 1.4038 - val_accuracy: 0.5018\n",
            "Epoch 22/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.3140 - accuracy: 0.5189 - val_loss: 1.2709 - val_accuracy: 0.5390\n",
            "Epoch 23/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.2846 - accuracy: 0.5306 - val_loss: 1.1892 - val_accuracy: 0.5667\n",
            "Epoch 24/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.2502 - accuracy: 0.5437 - val_loss: 1.2536 - val_accuracy: 0.5509\n",
            "Epoch 25/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.2253 - accuracy: 0.5545 - val_loss: 1.1664 - val_accuracy: 0.5774\n",
            "Epoch 26/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.1941 - accuracy: 0.5656 - val_loss: 1.1258 - val_accuracy: 0.5865\n",
            "Epoch 27/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.1652 - accuracy: 0.5780 - val_loss: 1.1886 - val_accuracy: 0.5796\n",
            "Epoch 28/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.1366 - accuracy: 0.5874 - val_loss: 1.0869 - val_accuracy: 0.6103\n",
            "Epoch 29/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.1208 - accuracy: 0.5945 - val_loss: 1.1076 - val_accuracy: 0.6020\n",
            "Epoch 30/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.0911 - accuracy: 0.6064 - val_loss: 1.0575 - val_accuracy: 0.6226\n",
            "Epoch 31/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.0648 - accuracy: 0.6170 - val_loss: 1.0338 - val_accuracy: 0.6295\n",
            "Epoch 32/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.0440 - accuracy: 0.6250 - val_loss: 1.1182 - val_accuracy: 0.5999\n",
            "Epoch 33/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.0218 - accuracy: 0.6328 - val_loss: 0.9841 - val_accuracy: 0.6529\n",
            "Epoch 34/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.0002 - accuracy: 0.6421 - val_loss: 1.0134 - val_accuracy: 0.6405\n",
            "Epoch 35/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9774 - accuracy: 0.6497 - val_loss: 1.0311 - val_accuracy: 0.6422\n",
            "Epoch 36/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9547 - accuracy: 0.6567 - val_loss: 0.9428 - val_accuracy: 0.6634\n",
            "Epoch 37/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9342 - accuracy: 0.6649 - val_loss: 0.9512 - val_accuracy: 0.6631\n",
            "Epoch 38/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9160 - accuracy: 0.6720 - val_loss: 0.9324 - val_accuracy: 0.6675\n",
            "Epoch 39/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.8902 - accuracy: 0.6792 - val_loss: 0.8994 - val_accuracy: 0.6838\n",
            "Epoch 40/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.8779 - accuracy: 0.6860 - val_loss: 0.8405 - val_accuracy: 0.7023\n",
            "Epoch 41/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.8548 - accuracy: 0.6955 - val_loss: 0.8846 - val_accuracy: 0.6846\n",
            "Epoch 42/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.8357 - accuracy: 0.7018 - val_loss: 0.8098 - val_accuracy: 0.7118\n",
            "Epoch 43/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.8179 - accuracy: 0.7090 - val_loss: 0.8497 - val_accuracy: 0.7001\n",
            "Epoch 44/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.7993 - accuracy: 0.7181 - val_loss: 0.8628 - val_accuracy: 0.6990\n",
            "Epoch 45/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.7874 - accuracy: 0.7187 - val_loss: 0.8246 - val_accuracy: 0.7118\n",
            "Epoch 46/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.7727 - accuracy: 0.7248 - val_loss: 0.9046 - val_accuracy: 0.6852\n",
            "Epoch 47/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.7473 - accuracy: 0.7334 - val_loss: 0.7611 - val_accuracy: 0.7327\n",
            "Epoch 48/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.7402 - accuracy: 0.7365 - val_loss: 0.7589 - val_accuracy: 0.7336\n",
            "Epoch 49/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.7250 - accuracy: 0.7437 - val_loss: 0.7679 - val_accuracy: 0.7312\n",
            "Epoch 50/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.7122 - accuracy: 0.7469 - val_loss: 0.7842 - val_accuracy: 0.7329\n",
            "Epoch 51/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.6967 - accuracy: 0.7536 - val_loss: 0.8040 - val_accuracy: 0.7170\n",
            "Epoch 52/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.6887 - accuracy: 0.7546 - val_loss: 0.7472 - val_accuracy: 0.7446\n",
            "Epoch 53/100\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6707 - accuracy: 0.7618 - val_loss: 0.7047 - val_accuracy: 0.7557\n",
            "Epoch 54/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.6563 - accuracy: 0.7692 - val_loss: 0.7054 - val_accuracy: 0.7559\n",
            "Epoch 55/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.6482 - accuracy: 0.7714 - val_loss: 0.6716 - val_accuracy: 0.7655\n",
            "Epoch 56/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.6342 - accuracy: 0.7764 - val_loss: 0.6867 - val_accuracy: 0.7654\n",
            "Epoch 57/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.6227 - accuracy: 0.7784 - val_loss: 0.7206 - val_accuracy: 0.7580\n",
            "Epoch 58/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.6152 - accuracy: 0.7809 - val_loss: 0.6891 - val_accuracy: 0.7653\n",
            "Epoch 59/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.6069 - accuracy: 0.7872 - val_loss: 0.6412 - val_accuracy: 0.7792\n",
            "Epoch 60/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.5927 - accuracy: 0.7918 - val_loss: 0.7050 - val_accuracy: 0.7530\n",
            "Epoch 61/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.5810 - accuracy: 0.7929 - val_loss: 0.6478 - val_accuracy: 0.7769\n",
            "Epoch 62/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.5743 - accuracy: 0.7967 - val_loss: 0.6351 - val_accuracy: 0.7826\n",
            "Epoch 63/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.5693 - accuracy: 0.7987 - val_loss: 0.6503 - val_accuracy: 0.7793\n",
            "Epoch 64/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.5570 - accuracy: 0.8050 - val_loss: 0.6731 - val_accuracy: 0.7726\n",
            "Epoch 65/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.5490 - accuracy: 0.8047 - val_loss: 0.6544 - val_accuracy: 0.7817\n",
            "Epoch 66/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.5400 - accuracy: 0.8093 - val_loss: 0.6616 - val_accuracy: 0.7777\n",
            "Epoch 67/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.5309 - accuracy: 0.8128 - val_loss: 0.7600 - val_accuracy: 0.7439\n",
            "Epoch 68/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.5200 - accuracy: 0.8156 - val_loss: 0.6205 - val_accuracy: 0.7936\n",
            "Epoch 69/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.5179 - accuracy: 0.8193 - val_loss: 0.5998 - val_accuracy: 0.7977\n",
            "Epoch 70/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.5109 - accuracy: 0.8206 - val_loss: 0.6158 - val_accuracy: 0.7934\n",
            "Epoch 71/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.5021 - accuracy: 0.8217 - val_loss: 0.5799 - val_accuracy: 0.8054\n",
            "Epoch 72/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4938 - accuracy: 0.8260 - val_loss: 0.7087 - val_accuracy: 0.7729\n",
            "Epoch 73/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4855 - accuracy: 0.8277 - val_loss: 0.5882 - val_accuracy: 0.8010\n",
            "Epoch 74/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4843 - accuracy: 0.8291 - val_loss: 0.6099 - val_accuracy: 0.7962\n",
            "Epoch 75/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4693 - accuracy: 0.8331 - val_loss: 0.5750 - val_accuracy: 0.8088\n",
            "Epoch 76/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4647 - accuracy: 0.8363 - val_loss: 0.6124 - val_accuracy: 0.8021\n",
            "Epoch 77/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4604 - accuracy: 0.8374 - val_loss: 0.5816 - val_accuracy: 0.8035\n",
            "Epoch 78/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4477 - accuracy: 0.8412 - val_loss: 0.5561 - val_accuracy: 0.8142\n",
            "Epoch 79/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4443 - accuracy: 0.8439 - val_loss: 0.6169 - val_accuracy: 0.7985\n",
            "Epoch 80/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4371 - accuracy: 0.8447 - val_loss: 0.6292 - val_accuracy: 0.7909\n",
            "Epoch 81/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4336 - accuracy: 0.8473 - val_loss: 0.6240 - val_accuracy: 0.7911\n",
            "Epoch 82/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4238 - accuracy: 0.8508 - val_loss: 0.6528 - val_accuracy: 0.7920\n",
            "Epoch 83/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4172 - accuracy: 0.8534 - val_loss: 0.5823 - val_accuracy: 0.8130\n",
            "Epoch 84/100\n",
            "391/391 [==============================] - 8s 22ms/step - loss: 0.4116 - accuracy: 0.8553 - val_loss: 0.6010 - val_accuracy: 0.8026\n",
            "Epoch 85/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4089 - accuracy: 0.8545 - val_loss: 0.5707 - val_accuracy: 0.8099\n",
            "Epoch 86/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3997 - accuracy: 0.8578 - val_loss: 0.5516 - val_accuracy: 0.8195\n",
            "Epoch 87/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3972 - accuracy: 0.8593 - val_loss: 0.6424 - val_accuracy: 0.7968\n",
            "Epoch 88/100\n",
            "391/391 [==============================] - 8s 22ms/step - loss: 0.3873 - accuracy: 0.8629 - val_loss: 0.6750 - val_accuracy: 0.7917\n",
            "Epoch 89/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3829 - accuracy: 0.8654 - val_loss: 0.5993 - val_accuracy: 0.8069\n",
            "Epoch 90/100\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3789 - accuracy: 0.8667 - val_loss: 0.5659 - val_accuracy: 0.8165\n",
            "Epoch 91/100\n",
            "391/391 [==============================] - 8s 22ms/step - loss: 0.3737 - accuracy: 0.8675 - val_loss: 0.5910 - val_accuracy: 0.8113\n",
            "Epoch 92/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3660 - accuracy: 0.8687 - val_loss: 0.5694 - val_accuracy: 0.8171\n",
            "Epoch 93/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3631 - accuracy: 0.8707 - val_loss: 0.6169 - val_accuracy: 0.8066\n",
            "Epoch 94/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3612 - accuracy: 0.8722 - val_loss: 0.5460 - val_accuracy: 0.8257\n",
            "Epoch 95/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3512 - accuracy: 0.8744 - val_loss: 0.5350 - val_accuracy: 0.8252\n",
            "Epoch 96/100\n",
            "391/391 [==============================] - 8s 22ms/step - loss: 0.3448 - accuracy: 0.8773 - val_loss: 0.5860 - val_accuracy: 0.8212\n",
            "Epoch 97/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3442 - accuracy: 0.8787 - val_loss: 0.5649 - val_accuracy: 0.8180\n",
            "Epoch 98/100\n",
            "391/391 [==============================] - 8s 22ms/step - loss: 0.3360 - accuracy: 0.8813 - val_loss: 0.5975 - val_accuracy: 0.8166\n",
            "Epoch 99/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3341 - accuracy: 0.8812 - val_loss: 0.5233 - val_accuracy: 0.8296\n",
            "Epoch 100/100\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3286 - accuracy: 0.8829 - val_loss: 0.5881 - val_accuracy: 0.8122\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa4434b0fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOE5QPW8KSHJ"
      },
      "source": [
        "若依照以上範例的設定，經過 20 輪訓練後，DNN 可以到達約 40% 的正確率，而 CNN 可以達到約 50% 的正確率。請更改 DNN 或是 CNN 的架構來改善模型。提示：\n",
        "\n",
        "1. 調整訓練輪數(`epochs`)\n",
        "2. 調整批次大小(`batch_size`)\n",
        "3. 調整學習速率(`learning_rate`)\n",
        "4. 增加層數\n",
        "5. 調整每層的神經元數量"
      ]
    }
  ]
}